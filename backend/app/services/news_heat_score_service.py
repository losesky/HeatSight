import math
import uuid
import asyncio
import jieba
import jieba.analyse
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from loguru import logger
import os
from pathlib import Path

from app.core.config import settings
from app.db.redis import redis_manager
from app.models.news_heat_score import NewsHeatScore
from app.schemas.news_heat_score import HeatScoreCreate, HeatScoreUpdate
from app.crud import news_heat_score
from app.services.heatlink_client import HeatLinkAPIClient

# è®¾ç½®NLTKæ•°æ®ç›®å½•
NLTK_DATA_DIR = Path(__file__).parent.parent.parent / "nltk_data"
os.environ['NLTK_DATA'] = str(NLTK_DATA_DIR)

try:
    import nltk
    from nltk.tokenize import RegexpTokenizer
    from nltk.corpus import stopwords
    
    # åˆ›å»ºåŸºæœ¬çš„åˆ†è¯å™¨
    word_tokenizer = RegexpTokenizer(r'\w+')
    
    # ç¡®ä¿å¿…è¦çš„NLTKæ•°æ®å·²ä¸‹è½½
    def ensure_nltk_resource(resource):
        try:
            if resource == 'stopwords':
                # æµ‹è¯•åœç”¨è¯åŠŸèƒ½
                stopwords.words('english')
        except LookupError:
            logger.info(f"æ­£åœ¨ä¸‹è½½NLTKèµ„æº: {resource}")
            nltk.download(resource, quiet=True, download_dir=str(NLTK_DATA_DIR))
    
    # åªä¸‹è½½åœç”¨è¯èµ„æº
    ensure_nltk_resource('stopwords')
    
    NLTK_AVAILABLE = True
    logger.info("âœ¨ NLTKåˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ NLTKåˆå§‹åŒ–å¤±è´¥: {e}")
    NLTK_AVAILABLE = False

# ç®—æ³•å¸¸é‡é…ç½®
BASELINE_FACTOR = 10  # åŸºå‡†ç³»æ•°ï¼Œç”¨äºå½’ä¸€åŒ–å…³é”®è¯åŒ¹é…åº¦
DECAY_FACTOR = 24.0  # æ—¶æ•ˆæ€§è¡°å‡å› å­ï¼ˆå°æ—¶ï¼‰
W_KEYWORD = 0.3  # å…³é”®è¯åŒ¹é…åº¦æƒé‡
W_RECENCY = 0.25  # æ—¶æ•ˆæ€§æƒé‡
W_PLATFORM = 0.15  # åŸå¹³å°çƒ­åº¦æƒé‡
W_CROSS_SOURCE = 0.2  # è·¨æºé¢‘ç‡æƒé‡
W_SOURCE = 0.1  # æ¥æºæƒé‡

# ç¼“å­˜é…ç½®
CACHE_PREFIX = "heatsight:heatscore"
CACHE_TTL = 3600  # 1å°æ—¶ç¼“å­˜æ—¶é—´


class NewsHeatScoreService:
    """Service for calculating and managing news heat scores."""

    def __init__(self):
        self.heatlink_client = HeatLinkAPIClient()
        
        # åŠ è½½åœç”¨è¯
        self._load_stopwords()
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        self._init_tokenizers()

    def _load_stopwords(self):
        """åŠ è½½ä¸­è‹±æ–‡åœç”¨è¯"""
        try:
            # åŠ è½½ä¸­æ–‡åœç”¨è¯
            self.cn_stopwords = set()
            # åˆ›å»ºä¸´æ—¶åœç”¨è¯æ–‡ä»¶
            self.cn_stopwords_file = os.path.join(
                os.path.dirname(__file__),
                "cn_stopwords.txt"
            )
            
            # å†™å…¥åŸºæœ¬çš„ä¸­æ–‡åœç”¨è¯
            basic_stopwords = {
                "çš„", "äº†", "å’Œ", "æ˜¯", "å°±", "éƒ½", "è€Œ", "åŠ", "ä¸", "ç€",
                "æˆ–", "ä¸€ä¸ª", "æ²¡æœ‰", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒä»¬", "è¿™ä¸ª",
                "é‚£ä¸ª", "è¿™äº›", "é‚£äº›", "è¿™æ ·", "é‚£æ ·", "ä¹‹", "çš„è¯", "è¯´",
                "æ—¶å€™", "æ˜¾ç¤º", "ä¸€äº›", "ç°åœ¨", "å·²ç»", "ä»€ä¹ˆ", "åªæ˜¯", "è¿˜æ˜¯",
                "å¯ä»¥", "è¿™", "é‚£", "åˆ", "ä¹Ÿ", "æœ‰", "åˆ°", "å¾ˆ", "æ¥", "å»",
                "æŠŠ", "è¢«", "è®©", "ä½†", "ä½†æ˜¯", "ç„¶å", "æ‰€ä»¥", "å› ä¸º", "ç”±äº",
                "æ‰€ä»¥", "å› æ­¤", "å¦‚æœ", "è™½ç„¶", "äºæ˜¯", "ä¸€ç›´", "å¹¶", "å¹¶ä¸”",
                "ä¸è¿‡", "ä¸", "æ²¡", "ä¸€", "åœ¨", "ä¸­", "ä¸º", "ä»¥", "èƒ½", "è¦"
            }
            
            # å°†åœç”¨è¯å†™å…¥æ–‡ä»¶
            with open(self.cn_stopwords_file, "w", encoding="utf-8") as f:
                f.write("\n".join(basic_stopwords))
            
            self.cn_stopwords.update(basic_stopwords)
            
            # åŠ è½½è‹±æ–‡åœç”¨è¯
            if NLTK_AVAILABLE:
                self.en_stopwords = set(stopwords.words('english'))
            else:
                # åŸºæœ¬è‹±æ–‡åœç”¨è¯
                self.en_stopwords = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for',
                                   'from', 'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on',
                                   'that', 'the', 'to', 'was', 'were', 'will', 'with'}
            
            logger.info("âœ¨ åœç”¨è¯åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ åœç”¨è¯åŠ è½½å¤±è´¥: {e}")
            self.cn_stopwords = set()
            self.en_stopwords = set()

    def _init_tokenizers(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        # åˆå§‹åŒ–jiebaåˆ†è¯
        import io
        import sys
        
        # æ•è·jiebaçš„stdoutè¾“å‡º
        stdout = sys.stdout
        sys.stdout = io.StringIO()
        
        try:
            # jiebaåˆå§‹åŒ–
            jieba.initialize()
            
            # è·å–æ•è·çš„è¾“å‡ºå¹¶è®°å½•åˆ°æ—¥å¿—
            output = sys.stdout.getvalue()
            if output:
                logger.debug(f"Jiebaåˆå§‹åŒ–è¾“å‡º:\n{output}")
        finally:
            # æ¢å¤æ ‡å‡†è¾“å‡º
            sys.stdout = stdout
        
        logger.info("âœ¨ åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")

    def _is_chinese(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºä¸­æ–‡"""
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦çš„æ•°é‡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æœ¬
        return chinese_chars / len(text) > 0.3 if text else False

    def _tokenize_text(self, text: str) -> List[str]:
        """æ ¹æ®æ–‡æœ¬è¯­è¨€é€‰æ‹©åˆé€‚çš„åˆ†è¯æ–¹æ³•"""
        if self._is_chinese(text):
            # ä¸­æ–‡åˆ†è¯
            words = list(jieba.cut(text))
            # è¿‡æ»¤ä¸­æ–‡åœç”¨è¯
            words = [w for w in words if w not in self.cn_stopwords and len(w.strip()) > 0]
        else:
            # è‹±æ–‡åˆ†è¯
            if NLTK_AVAILABLE:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯å™¨
                words = word_tokenizer.tokenize(text.lower())
            else:
                # ç®€å•çš„è‹±æ–‡åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
                words = text.lower().split()
            # è¿‡æ»¤è‹±æ–‡åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
            words = [w for w in words if w not in self.en_stopwords and len(w.strip()) > 0]
        
        return words

    async def _extract_keywords(self, title: str, content: str = "") -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä¸­è‹±æ–‡åˆ†è¯æŠ€æœ¯æå–æ–°é—»å…³é”®è¯å’ŒçŸ­è¯­"""
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼Œæ ‡é¢˜æƒé‡æ›´é«˜æ‰€ä»¥é‡å¤ä¸‰æ¬¡
        text = f"{title} {title} {title} {content}"
        
        result = []
        if self._is_chinese(text):
            # 1. æå–å…³é”®çŸ­è¯­ï¼ˆ2-3ä¸ªè¯çš„ç»„åˆï¼‰
            import jieba.analyse
            # è®¾ç½®åœç”¨è¯æ–‡ä»¶è·¯å¾„
            jieba.analyse.set_stop_words(self.cn_stopwords_file)
            
            # ä½¿ç”¨TextRankæå–å…³é”®çŸ­è¯­
            keywords = jieba.analyse.textrank(
                text,
                topK=10,  # æå–æ›´å¤šå…³é”®è¯ä»¥ä¾¿ç»„åˆ
                withWeight=True,
                allowPOS=('ns', 'n', 'vn', 'v', 'nr')  # å…è®¸åè¯ã€åŠ¨è¯ã€äººåã€åœ°å
            )
            
            # å°†å•ä¸ªå…³é”®è¯ç»„åˆæˆçŸ­è¯­
            words = list(jieba.cut(title))  # ä¸»è¦ä»æ ‡é¢˜ä¸­æå–
            phrases = []
            for i in range(len(words)-1):
                if len(words[i]) > 1 and len(words[i+1]) > 1:  # åªç»„åˆåŒå­—åŠä»¥ä¸Šçš„è¯
                    phrase = words[i] + words[i+1]
                    if 4 <= len(phrase) <= 8:  # æ§åˆ¶çŸ­è¯­é•¿åº¦
                        phrases.append(phrase)
            
            # æ·»åŠ å…³é”®è¯
            for word, weight in keywords:
                if (word not in self.cn_stopwords and 
                    len(word.strip()) > 1):  # åªä¿ç•™åŒå­—åŠä»¥ä¸Šçš„è¯
                    result.append({
                        "word": word,
                        "weight": float(weight),
                        "type": "keyword"
                    })
            
            # æ·»åŠ çŸ­è¯­
            for phrase in phrases[:5]:  # é™åˆ¶çŸ­è¯­æ•°é‡
                # è®¡ç®—çŸ­è¯­æƒé‡ï¼ˆåŸºäºå…¶åŒ…å«çš„å…³é”®è¯æƒé‡ï¼‰
                phrase_weight = 0
                for word, weight in keywords:
                    if word in phrase:
                        phrase_weight += weight
                result.append({
                    "word": phrase,
                    "weight": float(phrase_weight or 0.5),
                    "type": "phrase"
                })
            
            # 2. å°è¯•æå–ä¸»é¢˜ï¼ˆé€šè¿‡æ ‡é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ç»„åˆï¼‰
            if "ï¼š" in title or ":" in title:
                parts = title.replace(":", "ï¼š").split("ï¼š")
                if len(parts) >= 2:
                    topic = parts[0].strip()
                    if 4 <= len(topic) <= 20:  # æ§åˆ¶ä¸»é¢˜é•¿åº¦
                        result.append({
                            "word": topic,
                            "weight": 1.0,  # ä¸»é¢˜æƒé‡æœ€é«˜
                            "type": "topic"
                        })
        else:
            # è‹±æ–‡æ–‡æœ¬å¤„ç†
            words = self._tokenize_text(text)
            # è®¡ç®—è¯é¢‘
            from collections import Counter
            word_freq = Counter(words)
            total = sum(word_freq.values())
            
            # æå–å•è¯å’ŒçŸ­è¯­
            phrases = []
            for i in range(len(words)-1):
                if len(words[i]) > 2 and len(words[i+1]) > 2:  # å¿½ç•¥å¤ªçŸ­çš„å•è¯
                    phrase = words[i] + " " + words[i+1]
                    phrases.append(phrase)
            
            # æ·»åŠ å…³é”®è¯
            for word, count in word_freq.most_common(5):
                if word not in self.en_stopwords and len(word) > 2:
                    result.append({
                        "word": word,
                        "weight": float(count/total),
                        "type": "keyword"
                    })
            
            # æ·»åŠ çŸ­è¯­
            phrase_freq = Counter(phrases)
            for phrase, count in phrase_freq.most_common(3):
                result.append({
                    "word": phrase,
                    "weight": float(count/total),
                    "type": "phrase"
                })
        
        return result

    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦"""
        # åˆ†è¯
        words1 = set(self._tokenize_text(title1))
        words2 = set(self._tokenize_text(title2))
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0
        
        return intersection / union

    async def _find_similar_news(self, title: str, news_items: List[Dict]) -> int:
        """æŸ¥æ‰¾ä¸æŒ‡å®šæ ‡é¢˜ç›¸ä¼¼çš„æ–°é—»æ•°é‡"""
        similar_count = 0
        threshold = 0.6  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        for item in news_items:
            if item["title"] != title:  # æ’é™¤è‡ªèº«
                similarity = self._calculate_title_similarity(title, item["title"])
                if similarity > threshold:
                    similar_count += 1
        
        return similar_count

    async def _normalize_platform_score(
        self, metrics: Dict[str, Any], source_id: str
    ) -> float:
        """æ ‡å‡†åŒ–ä¸åŒå¹³å°çš„çƒ­åº¦æŒ‡æ ‡"""
        # ä¸åŒå¹³å°çš„åŸºå‡†å€¼
        platform_baselines = {
            "weibo": 10000,  # å¾®åšçƒ­æœåŸºå‡†å€¼
            "zhihu": 5000,   # çŸ¥ä¹çƒ­æ¦œåŸºå‡†å€¼
            "toutiao": 8000, # å¤´æ¡çƒ­æ¦œåŸºå‡†å€¼
            "default": 1000  # é»˜è®¤åŸºå‡†å€¼
        }
        
        # è·å–åŸå§‹çƒ­åº¦æŒ‡æ ‡ï¼ˆä¸åŒå¹³å°å¯èƒ½æœ‰ä¸åŒå­—æ®µï¼‰
        original_score = 0
        if "view_count" in metrics:
            original_score = metrics["view_count"]
        elif "like_count" in metrics:
            original_score = metrics["like_count"]
        elif "comment_count" in metrics:
            original_score = metrics["comment_count"]
        elif "heat" in metrics:
            original_score = metrics["heat"]
        
        # è·å–è¯¥å¹³å°çš„åŸºå‡†å€¼
        baseline = platform_baselines.get(source_id, platform_baselines["default"])
        
        # è®¡ç®—æ ‡å‡†åŒ–å¾—åˆ†ï¼ˆ0-100èŒƒå›´ï¼‰
        normalized_score = min(original_score / baseline * 100, 100)
        
        return normalized_score

    async def _calculate_cross_source_score(
        self, title: str, all_news_items: List[Dict]
    ) -> float:
        """è®¡ç®—è·¨æºé¢‘ç‡å¾—åˆ†"""
        # è·å–åŒ…å«è¯¥æ–°é—»çš„ä¸åŒæºæ•°é‡
        sources = set()
        for item in all_news_items:
            similarity = self._calculate_title_similarity(title, item["title"])
            if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                sources.add(item.get("source_id", ""))
        
        # è®¡ç®—å¾—åˆ†ï¼ˆå‡è®¾æœ€å¤šå‡ºç°åœ¨10ä¸ªæºä¸­ä¸ºæ»¡åˆ†ï¼‰
        score = min(len(sources) / 10 * 100, 100)
        
        return score

    async def _get_source_weight(self, source_id: str, session: AsyncSession) -> float:
        """è·å–æ¥æºæƒé‡"""
        # å¯ä»¥ä»æ•°æ®åº“æˆ–é…ç½®è·å–æ¥æºæƒé‡
        # è¿™é‡Œç®€åŒ–ä¸ºä¸€ä¸ªå›ºå®šçš„æƒé‡æ˜ å°„
        source_weights = {
            "weibo": 90,
            "zhihu": 85,
            "toutiao": 80,
            "sina": 75,
            "163": 70,
            "qq": 70,
            "sohu": 65,
            "ifeng": 65,
            "baidu": 90,
            "default": 50,
        }
        
        return source_weights.get(source_id, source_weights["default"])

    async def calculate_heat_score(
        self, 
        news_item: Dict[str, Any], 
        all_news_items: List[Dict[str, Any]],
        session: AsyncSession
    ) -> NewsHeatScore:
        """ä¸ºå•ä¸ªæ–°é—»é¡¹è®¡ç®—çƒ­åº¦åˆ†æ•°"""
        try:
            # æå–å…³é”®è¯
            keywords = await self._extract_keywords(
                news_item["title"], news_item.get("content", "")
            )
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦å¾—åˆ†ï¼ˆé€šè¿‡æœç´¢ç›¸ä¼¼æ–°é—»ï¼‰
            similar_count = 0
            for keyword in keywords[:3]:  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
                try:
                    search_response = await self.heatlink_client.get(
                        "news", params={"search": keyword["word"]}
                    )
                    if search_response and "items" in search_response:
                        similar_count += len(search_response["items"])
                except Exception as e:
                    logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            
            # å½’ä¸€åŒ–ç›¸ä¼¼æ–°é—»æ•°é‡ä¸º0-100åˆ†
            keyword_score = min(similar_count / BASELINE_FACTOR * 100, 100)
            
            # è®¡ç®—æ—¶æ•ˆæ€§å¾—åˆ†
            try:
                published_str = news_item["published_at"]
                if 'Z' in published_str:
                    published_str = published_str.replace('Z', '+00:00')
                elif '+' not in published_str and '-' not in published_str[10:]:
                    published_str = published_str + '+00:00'
                
                published_time = datetime.fromisoformat(published_str)
                if published_time.tzinfo is None:
                    published_time = published_time.replace(tzinfo=timezone.utc)
            except Exception as e:
                logger.error(f"è§£æå‘å¸ƒæ—¶é—´å¤±è´¥: {e}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå‘å¸ƒæ—¶é—´")
                published_time = datetime.now(timezone.utc)
            
            hours_passed = (datetime.now(timezone.utc) - published_time).total_seconds() / 3600
            recency_score = 100 * math.exp(-hours_passed / DECAY_FACTOR)
            
            # è®¡ç®—åŸå¹³å°çƒ­åº¦å¾—åˆ†
            platform_score = 0
            if "metrics" in news_item:
                platform_score = await self._normalize_platform_score(
                    news_item["metrics"], news_item["source_id"]
                )
            
            # è®¡ç®—è·¨æºé¢‘ç‡å¾—åˆ†
            cross_source_score = await self._calculate_cross_source_score(
                news_item["title"], all_news_items
            )
            
            # è·å–æ¥æºæƒé‡
            source_weight = await self._get_source_weight(news_item["source_id"], session)
            
            # æå–æˆ–æ¨æ–­åˆ†ç±»ä¿¡æ¯
            category = news_item.get("category")
            
            # å¦‚æœæ–°é—»é¡¹ä¸­æ²¡æœ‰ç›´æ¥æä¾›åˆ†ç±»ï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
            if not category and "meta_data" in news_item and isinstance(news_item["meta_data"], dict):
                category = news_item["meta_data"].get("category")
            
            # å¦‚æœè¿˜æ²¡æœ‰åˆ†ç±»ï¼Œæ ¹æ®æ¥æºå°è¯•æ¨æ–­
            if not category:
                # æ ¹æ®source_idæ¨æ–­åˆ†ç±»
                source_categories = {
                    "weibo": "social",
                    "zhihu": "knowledge",
                    "toutiao": "news",
                    "baidu": "search",
                    "bilibili": "video",
                    "douyin": "video",
                    "36kr": "technology",
                    "wallstreetcn": "finance",
                    "ithome": "technology",
                    "thepaper": "news",
                    "zaobao": "news",
                    "sina": "news",
                    "qq": "news",
                    "163": "news",
                    "sohu": "news",
                    "ifeng": "news",
                    "bbc_world": "world",
                    "bloomberg": "finance",
                    "hackernews": "technology",
                    "github": "technology",
                    "v2ex": "technology",
                    "kuaishou": "video"
                }
                category = source_categories.get(news_item["source_id"], "others")
                logger.debug(f"æ–°é—»[{news_item['id']}]æ²¡æœ‰åˆ†ç±»ä¿¡æ¯ï¼Œæ ¹æ®æ¥æº[{news_item['source_id']}]æ¨æ–­ä¸º: {category}")
            
            # ç»¼åˆè®¡ç®—æœ€ç»ˆçƒ­åº¦
            final_score = (
                (W_KEYWORD * keyword_score) +
                (W_RECENCY * recency_score) +
                (W_PLATFORM * platform_score) +
                (W_CROSS_SOURCE * cross_source_score) +
                (W_SOURCE * source_weight)
            )
            
            # å½’ä¸€åŒ–åˆ°0-100
            final_score = min(max(final_score, 0), 100)
            
            # åˆ›å»ºçƒ­åº¦è¯„åˆ†å¯¹è±¡
            heat_score = HeatScoreCreate(
                news_id=news_item["id"],
                source_id=news_item["source_id"],
                title=news_item["title"],
                url=news_item["url"],
                heat_score=final_score,
                relevance_score=keyword_score,
                recency_score=recency_score,
                popularity_score=platform_score,
                meta_data={
                    "cross_source_score": cross_source_score,
                    "source_weight": source_weight,
                    "keywords": [k["word"] for k in keywords[:5]],  # å°†å…³é”®è¯åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
                    "category": category  # æ·»åŠ åˆ†ç±»ä¿¡æ¯åˆ°meta_data
                },
                keywords=keywords,
                published_at=published_time,
            )
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            db_obj = await news_heat_score.create(session, heat_score)
            
            return db_obj
            
        except Exception as e:
            import traceback
            error_location = traceback.extract_tb(e.__traceback__)[-1]
            file_name = error_location.filename.split('/')[-1]
            line_no = error_location.lineno
            func_name = error_location.name
            
            error_msg = (
                f"æ–°é—»çƒ­åº¦è®¡ç®—å¤±è´¥:\n"
                f"é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                f"é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"å‘ç”Ÿä½ç½®: {file_name}:{line_no} in {func_name}\n"
                f"æ–°é—»ID: {news_item.get('id', 'N/A')}\n"
                f"æ ‡é¢˜: {news_item.get('title', 'N/A')}\n"
                f"æ¥æº: {news_item.get('source_id', 'N/A')}\n"
                f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
            )
            logger.error(error_msg)
            raise

    async def calculate_batch_heat_scores(
        self, news_items: List[Dict[str, Any]], session: AsyncSession
    ) -> Dict[str, NewsHeatScore]:
        """æ‰¹é‡è®¡ç®—çƒ­åº¦åˆ†æ•°"""
        logger.info(f"å¼€å§‹è®¡ç®—{len(news_items)}æ¡æ–°é—»çš„çƒ­åº¦åˆ†æ•°")
        
        results = {}
        for news_item in news_items:
            try:
                score_obj = await self.calculate_heat_score(
                    news_item, news_items, session
                )
                results[news_item["id"]] = score_obj
                logger.debug(f"æ–°é—»[{news_item['id']}]çƒ­åº¦è®¡ç®—å®Œæˆ: {score_obj.heat_score}")
            except Exception as e:
                import traceback
                error_location = traceback.extract_tb(e.__traceback__)[-1]
                file_name = error_location.filename.split('/')[-1]
                line_no = error_location.lineno
                func_name = error_location.name
                
                error_msg = (
                    f"æ–°é—»[{news_item['id']}]çƒ­åº¦è®¡ç®—å¤±è´¥:\n"
                    f"é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                    f"é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                    f"å‘ç”Ÿä½ç½®: {file_name}:{line_no} in {func_name}\n"
                    f"æ ‡é¢˜: {news_item.get('title', 'N/A')}\n"
                    f"æ¥æº: {news_item.get('source_id', 'N/A')}\n"
                    f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}"
                )
                logger.error(error_msg)
        
        logger.info(f"æ‰¹é‡çƒ­åº¦è®¡ç®—å®Œæˆï¼ŒæˆåŠŸ: {len(results)}, æ€»æ•°: {len(news_items)}")
        return results

    async def get_heat_scores(
        self, news_ids: List[str], session: AsyncSession
    ) -> Dict[str, float]:
        """è·å–å¤šä¸ªæ–°é—»çš„çƒ­åº¦åˆ†æ•°"""
        # å°è¯•ä»ç¼“å­˜è·å–
        # ä¸å†ä½¿ç”¨æ•´ä¸ªIDåˆ—è¡¨æ„å»ºç¼“å­˜é”®ï¼Œè€Œæ˜¯ä½¿ç”¨é•¿åº¦
        ids_count = len(news_ids)
        cache_key = f"{CACHE_PREFIX}:bulk:{uuid.uuid4().hex[:8]}:{ids_count}"
        
        cached_data = await redis_manager.get(cache_key)
        if cached_data:
            logger.debug(f"ä»ç¼“å­˜è·å–çƒ­åº¦åˆ†æ•°: {ids_count} æ¡")
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        logger.debug(f"ä»æ•°æ®åº“è·å–çƒ­åº¦åˆ†æ•°ï¼Œè¯·æ±‚ {ids_count} æ¡è®°å½•")
        scores_map = await news_heat_score.get_multi_by_news_ids(session, news_ids)
        
        # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
        result = {}
        for news_id in news_ids:
            if news_id in scores_map:
                result[news_id] = scores_map[news_id].heat_score
            else:
                result[news_id] = 0  # é»˜è®¤åˆ†æ•°
        
        # ç¼“å­˜ç»“æœ
        await redis_manager.set(cache_key, result, expire=CACHE_TTL)
        logger.debug(f"å·²å®Œæˆçƒ­åº¦åˆ†æ•°æŸ¥è¯¢ï¼Œè¿”å› {len(result)} æ¡ç»“æœ")
        
        return result

    async def get_detailed_heat_scores(
        self, news_ids: List[str], session: AsyncSession
    ) -> Dict[str, Any]:
        """è·å–å¤šä¸ªæ–°é—»çš„è¯¦ç»†çƒ­åº¦æ•°æ®"""
        # å°è¯•ä»ç¼“å­˜è·å–
        ids_count = len(news_ids)
        cache_key = f"{CACHE_PREFIX}:detailed:{uuid.uuid4().hex[:8]}:{ids_count}"
        
        cached_data = await redis_manager.get(cache_key)
        if cached_data:
            logger.debug(f"ä»ç¼“å­˜è·å–è¯¦ç»†çƒ­åº¦æ•°æ®: {ids_count} æ¡")
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        logger.debug(f"ä»æ•°æ®åº“è·å–è¯¦ç»†çƒ­åº¦æ•°æ®ï¼Œè¯·æ±‚ {ids_count} æ¡è®°å½•")
        scores_map = await news_heat_score.get_multi_by_news_ids(session, news_ids)
        
        # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
        result = {}
        for news_id in news_ids:
            if news_id in scores_map:
                result[news_id] = scores_map[news_id].to_dict()
        
        # ç¼“å­˜ç»“æœ
        await redis_manager.set(cache_key, result, expire=CACHE_TTL)
        logger.debug(f"å·²å®Œæˆè¯¦ç»†çƒ­åº¦æ•°æ®æŸ¥è¯¢ï¼Œè¿”å› {len(result)} æ¡ç»“æœ")
        
        return result

    async def get_top_news(
        self, 
        limit: int = 50, 
        skip: int = 0, 
        min_score: Optional[float] = 50.0,
        max_age_hours: Optional[int] = 72,
        category: Optional[str] = None,
        session: AsyncSession = None,
    ) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨æ–°é—»åˆ—è¡¨"""
        try:
            logger.info(f"è·å–çƒ­é—¨æ–°é—»åˆ—è¡¨: limit={limit}, skip={skip}, min_score={min_score}, max_age_hours={max_age_hours}, category={category}")
            
            # ä½¿ç”¨æ–°çš„å­—å…¸è¿”å›æ–¹æ³•ï¼Œé¿å… ORM æ¨¡å‹å’Œç›¸å…³å¼‚æ­¥é—®é¢˜
            news_list = await news_heat_score.get_top_news_as_dict(
                session, limit, skip, min_score, max_age_hours, category
            )
            
            logger.info(f"æˆåŠŸè·å–çƒ­é—¨æ–°é—»åˆ—è¡¨ï¼Œå…± {len(news_list)} æ¡è®°å½•")
            return news_list
        except Exception as e:
            logger.error(f"è·å–çƒ­é—¨æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}")
            # è®°å½•æ›´å¤šçš„é”™è¯¯ä¿¡æ¯ï¼Œæœ‰åŠ©äºè°ƒè¯•
            import traceback
            logger.error(traceback.format_exc())
            raise

    async def fetch_all_news_from_sources(self, sources: List[Dict]) -> List[Dict]:
        """ä»æ‰€æœ‰æºè·å–æ–°é—»"""
        logger.info(f"å¼€å§‹ä»{len(sources)}ä¸ªæºè·å–æ–°é—»")
        
        # é™ä½å¹¶å‘è¯·æ±‚æ•°é‡ä»¥å‡è½»ç³»ç»Ÿè´Ÿæ‹…
        max_concurrent = 3  # ä»5é™ä½åˆ°3
        all_news_items = []
        
        # åˆ†æ‰¹å¤„ç†æºï¼Œé¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
        for i in range(0, len(sources), max_concurrent):
            batch_sources = sources[i:i+max_concurrent]
            tasks = []
            
            for source in batch_sources:
                # é€‚é…ä¸åŒçš„APIè¿”å›æ ¼å¼ï¼Œå°è¯•å¤šç§å¯èƒ½çš„IDå­—æ®µå
                source_id = None
                for id_field in ["source_id", "id", "key", "name"]:
                    if id_field in source:
                        source_id = source[id_field]
                        break
                
                if not source_id:
                    logger.warning(f"è·³è¿‡æ²¡æœ‰IDçš„æº: {source}")
                    continue
                
                # è®¾ç½®ä»»åŠ¡è¶…æ—¶ä»¥é¿å…æŸäº›æºé•¿æ—¶é—´æ— å“åº”å¯¼è‡´æ•´ä½“é˜»å¡
                task = asyncio.create_task(
                    asyncio.wait_for(
                        self.heatlink_client.get(f"external/source/{source_id}"),
                        timeout=10  # æ·»åŠ 10ç§’è¶…æ—¶
                    )
                )
                tasks.append((source_id, task))
            
            # ç­‰å¾…è¿™ä¸€æ‰¹ä»»åŠ¡å®Œæˆ
            for source_id, task in tasks:
                try:
                    source_data = await task
                    news_items = []
                    
                    # å°è¯•ä»ä¸åŒçš„é”®è·å–æ–°é—»é¡¹
                    if source_data:
                        # 1. é¦–å…ˆå°è¯•ä»'news'é”®è·å–ï¼ˆå½“å‰APIæ ¼å¼ï¼‰
                        if isinstance(source_data, dict) and "news" in source_data:
                            news_items = source_data["news"]
                            logger.info(f"æº {source_id} è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                        
                        # 2. å¦‚æœæ²¡æœ‰newsé”®ï¼Œå°è¯•ä»'items'é”®è·å–ï¼ˆæ—§æ ¼å¼ï¼‰
                        elif isinstance(source_data, dict) and "items" in source_data:
                            news_items = source_data["items"]
                            logger.info(f"æº {source_id} è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                        
                        # 3. å¦‚æœAPIç›´æ¥è¿”å›äº†åˆ—è¡¨
                        elif isinstance(source_data, list):
                            news_items = source_data
                            logger.info(f"æº {source_id} è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                        
                        # ä¸ºæ¯ä¸ªæ–°é—»é¡¹æ·»åŠ source_id
                        for item in news_items:
                            item["source_id"] = source_id
                        
                        # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                        all_news_items.extend(news_items)
                    else:
                        logger.warning(f"ä»æº[{source_id}]è·å–åˆ°ç©ºæ•°æ®")
                except asyncio.TimeoutError:
                    logger.error(f"ä»æº[{source_id}]è·å–æ–°é—»è¶…æ—¶")
                except Exception as e:
                    logger.error(f"ä»æº[{source_id}]è·å–æ–°é—»å¤±è´¥: {e}")
            
            # æ·»åŠ çŸ­æš‚æš‚åœï¼Œè®©å…¶ä»–ä»»åŠ¡æœ‰æœºä¼šæ‰§è¡Œ
            await asyncio.sleep(0.1)
        
        logger.info(f"å…±è·å–åˆ° {len(all_news_items)} æ¡æ–°é—»")
        return all_news_items

    async def update_all_heat_scores(self, session: AsyncSession):
        """æ›´æ–°æ‰€æœ‰æ–°é—»çƒ­åº¦åˆ†æ•°"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰æ–°é—»çƒ­åº¦åˆ†æ•°")
        
        try:
            # 1. è·å–æ‰€æœ‰æ–°é—»æºï¼Œè®¾ç½®è¶…æ—¶é˜²æ­¢é˜»å¡
            try:
                sources_data = await asyncio.wait_for(
                    self.heatlink_client.get_sources(force_update=True),
                    timeout=15  # 15ç§’è¶…æ—¶
                )
            except asyncio.TimeoutError:
                logger.error("âŒ è·å–æ–°é—»æºè¶…æ—¶ï¼Œä»»åŠ¡ç»ˆæ­¢")
                return []
                
            # å¤„ç†APIè¿”å›å€¼å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸çš„æƒ…å†µ
            if isinstance(sources_data, dict):
                sources = sources_data.get("sources", [])
            else:
                # å¦‚æœAPIç›´æ¥è¿”å›åˆ—è¡¨ï¼Œå°±ç›´æ¥ä½¿ç”¨
                sources = sources_data
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æº
            if not sources:
                logger.warning("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„æ–°é—»æºï¼Œä»»åŠ¡ç»ˆæ­¢")
                return []
                
            logger.info(f"ğŸ“Š æˆåŠŸè·å– {len(sources)} ä¸ªæ–°é—»æº")
            
            # 2. è·å–æ‰€æœ‰æ–°é—»
            all_news_items = await self.fetch_all_news_from_sources(sources)
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œç›´æ¥è¿”å›
            if not all_news_items:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ–°é—»ï¼Œä»»åŠ¡ç»ˆæ­¢")
                return []
                
            # 3. è®¡ç®—çƒ­åº¦è¯„åˆ†
            heat_scores = await self.calculate_batch_heat_scores(all_news_items, session)
            
            logger.info(f"âœ¨ çƒ­åº¦åˆ†æ•°æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(heat_scores)} æ¡æ–°é—»")
            return heat_scores
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°çƒ­åº¦åˆ†æ•°å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å¼•å‘å¼‚å¸¸ï¼Œé¿å…ä¸­æ–­è°ƒåº¦å™¨
            return []

    async def update_keyword_heat(self, session: AsyncSession):
        """æ›´æ–°å…³é”®è¯çƒ­åº¦"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°å…³é”®è¯çƒ­åº¦")
        
        try:
            # è·å–æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„çƒ­é—¨æ–°é—»
            heat_scores = await news_heat_score.get_top_heat_scores(
                session, 
                limit=1000,  # å¢åŠ è·å–çš„æ–°é—»æ•°é‡åˆ°1000æ¡
                skip=0,
                min_score=20,  # é™ä½çƒ­åº¦åˆ†æ•°é˜ˆå€¼åˆ°20
                max_age_hours=12  # åªè·å–æœ€è¿‘12å°æ—¶çš„æ–°é—»
            )
            
            # æå–æ‰€æœ‰å…³é”®è¯å¹¶è®¡ç®—é¢‘ç‡
            keyword_counts = {}
            for score in heat_scores:
                for keyword_item in score.keywords:
                    word = keyword_item.get("word")
                    weight = keyword_item.get("weight", 0.5)
                    word_type = keyword_item.get("type", "keyword")
                    
                    if word:
                        if word not in keyword_counts:
                            keyword_counts[word] = {
                                "count": 0,
                                "total_weight": 0,
                                "total_heat": 0,
                                "sources": set(),
                                "type": word_type
                            }
                        
                        keyword_counts[word]["count"] += 1
                        keyword_counts[word]["total_weight"] += weight
                        keyword_counts[word]["total_heat"] += score.heat_score
                        keyword_counts[word]["sources"].add(score.source_id)
            
            # è®¡ç®—å¹¶å­˜å‚¨å…³é”®è¯çƒ­åº¦
            keyword_heat = []
            for word, data in keyword_counts.items():
                # æ ¹æ®ç±»å‹è°ƒæ•´è¿‡æ»¤æ¡ä»¶
                should_include = False
                if data["type"] == "topic":
                    # ä¸»é¢˜è‡³å°‘åœ¨2ä¸ªæ¥æºå‡ºç°
                    should_include = len(data["sources"]) >= 2
                elif data["type"] == "phrase":
                    # çŸ­è¯­è‡³å°‘åœ¨2ä¸ªæ¥æºå‡ºç°ä¸”å‡ºç°2æ¬¡ä»¥ä¸Š
                    should_include = len(data["sources"]) >= 2 and data["count"] >= 2
                else:
                    # å…³é”®è¯è‡³å°‘åœ¨3ä¸ªæ¥æºå‡ºç°ä¸”å‡ºç°1æ¬¡ä»¥ä¸Š
                    should_include = len(data["sources"]) >= 3 and data["count"] >= 1
                
                if should_include:
                    # æ ¹æ®ç±»å‹è°ƒæ•´çƒ­åº¦è®¡ç®—
                    base_heat = (
                        data["count"] * 
                        (data["total_weight"] / data["count"]) * 
                        (data["total_heat"] / data["count"]) * 
                        len(data["sources"])
                    )
                    
                    # ä¸åŒç±»å‹çš„æƒé‡è°ƒæ•´
                    if data["type"] == "topic":
                        heat = base_heat / 500  # ä¸»é¢˜çƒ­åº¦æƒé‡æ›´é«˜
                    elif data["type"] == "phrase":
                        heat = base_heat / 750  # çŸ­è¯­çƒ­åº¦æƒé‡é€‚ä¸­
                    else:
                        heat = base_heat / 1000  # å…³é”®è¯çƒ­åº¦æƒé‡æ ‡å‡†
                    
                    keyword_heat.append({
                        "keyword": word,
                        "heat": min(heat, 100),  # ä¸Šé™100
                        "count": data["count"],
                        "sources": list(data["sources"]),
                        "type": data["type"],  # æ·»åŠ ç±»å‹ä¿¡æ¯
                        "updated_at": datetime.now(timezone.utc).isoformat()
                    })
            
            # æ›´æ–°åˆ°Redisç¼“å­˜
            if keyword_heat:
                # æŒ‰çƒ­åº¦æ’åº
                keyword_heat.sort(key=lambda x: x["heat"], reverse=True)
                # ä¿ç•™å‰300ä¸ªå…³é”®è¯/çŸ­è¯­/ä¸»é¢˜
                top_keywords = keyword_heat[:300]
                
                # å­˜å‚¨åˆ°Redis
                cache_key = f"{CACHE_PREFIX}:keywords"
                await redis_manager.set(cache_key, top_keywords, expire=CACHE_TTL * 2)
                
                logger.info(f"âœ¨ å…³é”®è¯çƒ­åº¦æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(top_keywords)} ä¸ªå…³é”®è¯/çŸ­è¯­/ä¸»é¢˜")
                return top_keywords
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°è¶³å¤Ÿçš„å…³é”®è¯æ•°æ®")
                return []
        
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å…³é”®è¯çƒ­åº¦å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    async def update_source_weights(self, session: AsyncSession):
        """æ›´æ–°æ¥æºæƒé‡"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°æ¥æºæƒé‡")
        
        try:
            # è·å–æ‰€æœ‰æ–°é—»æº
            sources_data = await self.heatlink_client.get_sources(force_update=True)
            
            # å¤„ç†APIè¿”å›å€¼å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸çš„æƒ…å†µ
            if isinstance(sources_data, dict):
                sources = sources_data.get("sources", [])
            else:
                # å¦‚æœAPIç›´æ¥è¿”å›åˆ—è¡¨ï¼Œå°±ç›´æ¥ä½¿ç”¨
                sources = sources_data
            
            logger.info(f"ğŸ“Š æˆåŠŸè·å–åˆ° {len(sources)} ä¸ªæ–°é—»æº")
            
            # åˆå§‹åŒ–æ¥æºæƒé‡æ•°æ®
            source_stats = {}
            
            # ä¸ºæ¯ä¸ªæ¥æºè·å–æœ€è¿‘çš„æ–°é—»
            for source in sources:
                source_id = source.get("source_id") or source.get("id")
                if not source_id:
                    logger.warning("âš ï¸ è·³è¿‡æ²¡æœ‰æœ‰æ•ˆIDçš„æº")
                    continue
                    
                try:
                    # è·å–è¯¥æ¥æºçš„æœ€æ–°æ–°é—»
                    source_news = await self.heatlink_client.get_source(source_id, force_update=True)
                    
                    # å°è¯•ä»ä¸åŒçš„é”®è·å–æ–°é—»é¡¹
                    news_items = []
                    
                    # å…ˆå°è¯•ä»'news'é”®è·å–ï¼Œè¿™æ˜¯APIå½“å‰çš„æ ¼å¼
                    if isinstance(source_news, dict) and "news" in source_news:
                        news_items = source_news.get("news", [])
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå†å°è¯•ä»'items'é”®è·å–ï¼ˆæ—§æ ¼å¼çš„å…¼å®¹ï¼‰
                    elif isinstance(source_news, dict) and "items" in source_news:
                        news_items = source_news.get("items", [])
                    
                    # æˆ–è€…APIç›´æ¥è¿”å›äº†åˆ—è¡¨
                    elif isinstance(source_news, list):
                        news_items = source_news
                    
                    # ç»Ÿè®¡è¯¥æ¥æºçš„æ•°æ®
                    if news_items:
                        avg_engagement = 0
                        total_items = len(news_items)
                        
                        # åˆ†ææ–°é—»é¡¹çš„äº’åŠ¨æ•°æ®
                        for item in news_items:
                            metrics = item.get("metrics", {})
                            # æ›´æ–°äº’åŠ¨æ•°æ®è®¡ç®—é€»è¾‘
                            view_count = float(metrics.get("view_count", 0))
                            like_count = float(metrics.get("like_count", 0))
                            comment_count = float(metrics.get("comment_count", 0))
                            share_count = float(metrics.get("share_count", 0))
                            
                            # è®¡ç®—äº’åŠ¨åˆ†æ•°ï¼Œä¸å†æ·»åŠ åŸºç¡€åˆ†
                            engagement = (
                                view_count * 1 +      # æµè§ˆé‡æƒé‡
                                like_count * 3 +      # ç‚¹èµæƒé‡
                                comment_count * 5 +   # è¯„è®ºæƒé‡
                                share_count * 10      # åˆ†äº«æƒé‡
                            )
                            
                            # æ ¹æ®æ¥æºç±»å‹è®¾ç½®ä¸åŒçš„åŸºå‡†å€¼
                            baseline = {
                                "weibo": 10000,      # å¾®åšåŸºå‡†å€¼é«˜
                                "zhihu": 5000,       # çŸ¥ä¹åŸºå‡†å€¼ä¸­ç­‰
                                "bilibili": 3000,    # Bç«™åŸºå‡†å€¼é€‚ä¸­
                                "toutiao": 8000,     # å¤´æ¡åŸºå‡†å€¼è¾ƒé«˜
                                "36kr": 2000,        # 36æ°ªåŸºå‡†å€¼è¾ƒä½
                                "default": 1000      # é»˜è®¤åŸºå‡†å€¼æœ€ä½
                            }.get(source_id, 1000)
                            
                            # æ ‡å‡†åŒ–äº’åŠ¨åˆ†æ•°
                            normalized_engagement = min((engagement / baseline) * 100, 100)
                            avg_engagement += normalized_engagement
                        
                        if total_items > 0:
                            avg_engagement /= total_items
                        
                        # è®¡ç®—æ›´æ–°é¢‘ç‡åˆ†æ•°
                        update_frequency = 50  # é»˜è®¤å€¼
                        if total_items >= 5:  # è‡³å°‘éœ€è¦5ç¯‡æ–‡ç« æ‰èƒ½è®¡ç®—æ›´æ–°é¢‘ç‡
                            try:
                                # è·å–æœ€æ–°çš„5ç¯‡æ–‡ç« çš„æ—¶é—´
                                timestamps = []
                                for item in news_items[:5]:
                                    pub_str = item["published_at"]
                                    
                                    # æ ‡å‡†åŒ–æ—¶é—´å­—ç¬¦ä¸²
                                    if 'Z' in pub_str:
                                        pub_str = pub_str.replace('Z', '+00:00')
                                    elif '+' not in pub_str and '-' not in pub_str[10:]:
                                        pub_str = pub_str + '+00:00'
                                    
                                    # è§£ææ—¶é—´
                                    pub_time = datetime.fromisoformat(pub_str)
                                    if pub_time.tzinfo is None:
                                        pub_time = pub_time.replace(tzinfo=timezone.utc)
                                    
                                    timestamps.append(pub_time)
                                
                                # è®¡ç®—å¹³å‡æ—¶é—´é—´éš”ï¼ˆå°æ—¶ï¼‰
                                intervals = []
                                for i in range(len(timestamps)-1):
                                    interval = (timestamps[i] - timestamps[i+1]).total_seconds() / 3600
                                    intervals.append(interval)
                                
                                avg_interval = sum(intervals) / len(intervals)
                                
                                # æ ¹æ®å¹³å‡æ›´æ–°é—´éš”è®¡ç®—é¢‘ç‡åˆ†æ•°
                                if avg_interval <= 0.0833:     # å¹³å‡5åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡ (5/60=0.0833å°æ—¶)
                                    update_frequency = 100
                                elif avg_interval <= 0.1667:   # å¹³å‡10åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡ (10/60=0.1667å°æ—¶)
                                    update_frequency = 90
                                elif avg_interval <= 0.5:      # å¹³å‡30åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡ (30/60=0.5å°æ—¶)
                                    update_frequency = 80
                                elif avg_interval <= 1:        # å¹³å‡60åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
                                    update_frequency = 70
                                elif avg_interval <= 2:        # å¹³å‡120åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡
                                    update_frequency = 60
                                elif avg_interval <= 4:        # å¹³å‡4å°æ—¶æ›´æ–°ä¸€æ¬¡
                                    update_frequency = 50
                                else:                         # æ›´æ–°è¾ƒæ…¢
                                    update_frequency = 40
                            except Exception as e:
                                logger.warning(f"âš ï¸ è®¡ç®—æ›´æ–°é¢‘ç‡å¤±è´¥: {e}")
                                import traceback
                                logger.debug(traceback.format_exc())
                        
                        # æ›´æ–°æ¥æºæƒé‡è®¡ç®—å…¬å¼
                        # åŸºç¡€æƒé‡ï¼šæ ¹æ®æ¥æºç±»å‹åˆ†é…
                        base_weight = {
                            "weibo": 90,  # å¾®åšçƒ­æœ
                            "zhihu": 85,  # çŸ¥ä¹çƒ­æ¦œ
                            "toutiao": 85,  # å¤´æ¡çƒ­æ¦œ
                            "baidu": 85,  # ç™¾åº¦çƒ­æœ
                            "bilibili": 80,  # Bç«™çƒ­é—¨
                            "douyin": 80,  # æŠ–éŸ³çƒ­ç‚¹
                            "kuaishou": 75,  # å¿«æ‰‹çƒ­ç‚¹
                            "36kr": 75,  # ç§‘æŠ€åª’ä½“
                            "wallstreetcn": 75,  # åå°”è¡—è§é—»
                            "thepaper": 70,  # æ¾æ¹ƒæ–°é—»
                            "ithome": 70,  # ITä¹‹å®¶
                            "zaobao": 70,  # è”åˆæ—©æŠ¥
                            "bbc_world": 85,  # BBCå›½é™…æ–°é—»
                            "bloomberg": 85,  # å½­åšæ–°é—»
                            "v2ex": 65,  # ç§‘æŠ€ç¤¾åŒº
                            "hackernews": 70,  # ç§‘æŠ€æ–°é—»
                            "github": 60,  # å¼€å‘å¹³å°
                        }.get(source_id, 50)  # å…¶ä»–æ¥æºé»˜è®¤50åˆ†
                        
                        # åŠ¨æ€æƒé‡ï¼šåŸºäºäº’åŠ¨åº¦å’Œæ›´æ–°é¢‘ç‡
                        engagement_score = min((avg_engagement / 1000), 100)  # äº’åŠ¨åº¦å¾—åˆ†
                        
                        # ç»¼åˆè®¡ç®—æœ€ç»ˆæƒé‡
                        source_weight = (
                            base_weight * 0.5 +  # åŸºç¡€æƒé‡å 50%
                            engagement_score * 0.3 +  # äº’åŠ¨åº¦å 30%
                            update_frequency * 0.2  # æ›´æ–°é¢‘ç‡å 20%
                        )
                        
                        # ç¡®ä¿æƒé‡åœ¨åˆç†èŒƒå›´å†…
                        source_weight = max(min(source_weight, 100), 10)
                        
                        # å­˜å‚¨æ¥æºç»Ÿè®¡æ•°æ®
                        source_stats[source_id] = {
                            "weight": source_weight,
                            "avg_engagement": avg_engagement,
                            "update_frequency": update_frequency,
                            "item_count": total_items,
                            "updated_at": datetime.now(timezone.utc).isoformat()
                        }
                        logger.debug(f"âœ… æˆåŠŸå¤„ç†æº '{source_id}', æƒé‡={source_weight:.2f}")
                    else:
                        logger.warning(f"âš ï¸ æº '{source_id}' æ²¡æœ‰è¿”å›æ–°é—»æ•°æ®")
                
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ¥æº {source_id} å¤±è´¥: {e}")
            
            # å­˜å‚¨æ‰€æœ‰æ¥æºæƒé‡åˆ°Redis
            if source_stats:
                cache_key = f"{CACHE_PREFIX}:source_weights"
                await redis_manager.set(cache_key, source_stats, expire=CACHE_TTL * 24)  # ç¼“å­˜24å°æ—¶
                
                logger.info(f"âœ¨ æ¥æºæƒé‡æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(source_stats)} ä¸ªæ¥æº")
                return source_stats
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„æ¥æºæ•°æ®")
                return {}
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¥æºæƒé‡å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise


# åˆ›å»ºæœåŠ¡å®ä¾‹
heat_score_service = NewsHeatScoreService() 