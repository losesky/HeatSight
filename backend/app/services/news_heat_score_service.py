import math
import uuid
import asyncio
import jieba
import jieba.analyse
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from loguru import logger
import os
from pathlib import Path

from app.core.config import settings
from app.db.redis import redis_manager
from app.models.news_heat_score import NewsHeatScore
from app.schemas.news_heat_score import HeatScoreCreate, HeatScoreUpdate
from app.crud import news_heat_score
from app.services.heatlink_client import HeatLinkAPIClient

# è®¾ç½®NLTKæ•°æ®ç›®å½•
NLTK_DATA_DIR = Path(__file__).parent.parent.parent / "nltk_data"
os.environ['NLTK_DATA'] = str(NLTK_DATA_DIR)

try:
    import nltk
    from nltk.tokenize import RegexpTokenizer
    from nltk.corpus import stopwords
    
    # åˆ›å»ºåŸºæœ¬çš„åˆ†è¯å™¨
    word_tokenizer = RegexpTokenizer(r'\w+')
    
    # ç¡®ä¿å¿…è¦çš„NLTKæ•°æ®å·²ä¸‹è½½
    def ensure_nltk_resource(resource):
        try:
            if resource == 'stopwords':
                # æµ‹è¯•åœç”¨è¯åŠŸèƒ½
                stopwords.words('english')
        except LookupError:
            logger.info(f"æ­£åœ¨ä¸‹è½½NLTKèµ„æº: {resource}")
            nltk.download(resource, quiet=True, download_dir=str(NLTK_DATA_DIR))
    
    # åªä¸‹è½½åœç”¨è¯èµ„æº
    ensure_nltk_resource('stopwords')
    
    NLTK_AVAILABLE = True
    logger.info("âœ¨ NLTKåˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸ NLTKåˆå§‹åŒ–å¤±è´¥: {e}")
    NLTK_AVAILABLE = False

# ç®—æ³•å¸¸é‡é…ç½®
BASELINE_FACTOR = 10  # åŸºå‡†ç³»æ•°ï¼Œç”¨äºå½’ä¸€åŒ–å…³é”®è¯åŒ¹é…åº¦
DECAY_FACTOR = 24.0  # æ—¶æ•ˆæ€§è¡°å‡å› å­ï¼ˆå°æ—¶ï¼‰
W_KEYWORD = 0.3  # å…³é”®è¯åŒ¹é…åº¦æƒé‡
W_RECENCY = 0.25  # æ—¶æ•ˆæ€§æƒé‡
W_PLATFORM = 0.15  # åŸå¹³å°çƒ­åº¦æƒé‡
W_CROSS_SOURCE = 0.2  # è·¨æºé¢‘ç‡æƒé‡
W_SOURCE = 0.1  # æ¥æºæƒé‡

# ç¼“å­˜é…ç½®
CACHE_PREFIX = "heatsight:heatscore"
CACHE_TTL = 3600  # 1å°æ—¶ç¼“å­˜æ—¶é—´


class NewsHeatScoreService:
    """Service for calculating and managing news heat scores."""

    def __init__(self):
        self.heatlink_client = HeatLinkAPIClient()
        
        # åŠ è½½åœç”¨è¯
        self._load_stopwords()
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        self._init_tokenizers()

    def _load_stopwords(self):
        """åŠ è½½ä¸­è‹±æ–‡åœç”¨è¯"""
        try:
            # åŠ è½½ä¸­æ–‡åœç”¨è¯
            self.cn_stopwords = set()
            # å¦‚æœæœ‰ä¸­æ–‡åœç”¨è¯æ–‡ä»¶ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ è½½
            # with open("path/to/cn_stopwords.txt", "r", encoding="utf-8") as f:
            #     self.cn_stopwords = set([line.strip() for line in f])
            
            # åŠ è½½è‹±æ–‡åœç”¨è¯
            if NLTK_AVAILABLE:
                self.en_stopwords = set(stopwords.words('english'))
            else:
                # åŸºæœ¬è‹±æ–‡åœç”¨è¯
                self.en_stopwords = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for',
                                   'from', 'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on',
                                   'that', 'the', 'to', 'was', 'were', 'will', 'with'}
            
            logger.info("âœ¨ åœç”¨è¯åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ åœç”¨è¯åŠ è½½å¤±è´¥: {e}")
            self.cn_stopwords = set()
            self.en_stopwords = set()

    def _init_tokenizers(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        # åˆå§‹åŒ–jiebaåˆ†è¯
        import io
        import sys
        
        # æ•è·jiebaçš„stdoutè¾“å‡º
        stdout = sys.stdout
        sys.stdout = io.StringIO()
        
        try:
            # jiebaåˆå§‹åŒ–
            jieba.initialize()
            
            # è·å–æ•è·çš„è¾“å‡ºå¹¶è®°å½•åˆ°æ—¥å¿—
            output = sys.stdout.getvalue()
            if output:
                logger.debug(f"Jiebaåˆå§‹åŒ–è¾“å‡º:\n{output}")
        finally:
            # æ¢å¤æ ‡å‡†è¾“å‡º
            sys.stdout = stdout
        
        logger.info("âœ¨ åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")

    def _is_chinese(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸»è¦ä¸ºä¸­æ–‡"""
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦çš„æ•°é‡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡æ–‡æœ¬
        return chinese_chars / len(text) > 0.3 if text else False

    def _tokenize_text(self, text: str) -> List[str]:
        """æ ¹æ®æ–‡æœ¬è¯­è¨€é€‰æ‹©åˆé€‚çš„åˆ†è¯æ–¹æ³•"""
        if self._is_chinese(text):
            # ä¸­æ–‡åˆ†è¯
            words = list(jieba.cut(text))
            # è¿‡æ»¤ä¸­æ–‡åœç”¨è¯
            words = [w for w in words if w not in self.cn_stopwords and len(w.strip()) > 0]
        else:
            # è‹±æ–‡åˆ†è¯
            if NLTK_AVAILABLE:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯å™¨
                words = word_tokenizer.tokenize(text.lower())
            else:
                # ç®€å•çš„è‹±æ–‡åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
                words = text.lower().split()
            # è¿‡æ»¤è‹±æ–‡åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
            words = [w for w in words if w not in self.en_stopwords and len(w.strip()) > 0]
        
        return words

    async def _extract_keywords(self, title: str, content: str = "") -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä¸­è‹±æ–‡åˆ†è¯æŠ€æœ¯æå–æ–°é—»å…³é”®è¯"""
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼Œæ ‡é¢˜æƒé‡æ›´é«˜æ‰€ä»¥é‡å¤ä¸€æ¬¡
        text = f"{title} {title} {content}"
        
        if self._is_chinese(text):
            # ä¸­æ–‡å…³é”®è¯æå–
            keywords = jieba.analyse.textrank(text, topK=5, withWeight=True)
        else:
            # è‹±æ–‡å…³é”®è¯æå–
            words = self._tokenize_text(text)
            # è®¡ç®—è¯é¢‘
            from collections import Counter
            word_freq = Counter(words)
            total = sum(word_freq.values())
            # è½¬æ¢ä¸ºå¸¦æƒé‡çš„å…³é”®è¯åˆ—è¡¨
            keywords = [(word, count/total) for word, count in word_freq.most_common(5)]
        
        # è½¬æ¢ä¸ºæ‰€éœ€çš„æ•°æ®ç»“æ„
        result = []
        for word, weight in keywords:
            if (word not in self.cn_stopwords and 
                word not in self.en_stopwords and 
                len(word.strip()) > 0):
                result.append({"word": word, "weight": float(weight)})
        
        return result

    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦"""
        # åˆ†è¯
        words1 = set(self._tokenize_text(title1))
        words2 = set(self._tokenize_text(title2))
        
        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0
        
        return intersection / union

    async def _find_similar_news(self, title: str, news_items: List[Dict]) -> int:
        """æŸ¥æ‰¾ä¸æŒ‡å®šæ ‡é¢˜ç›¸ä¼¼çš„æ–°é—»æ•°é‡"""
        similar_count = 0
        threshold = 0.6  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
        for item in news_items:
            if item["title"] != title:  # æ’é™¤è‡ªèº«
                similarity = self._calculate_title_similarity(title, item["title"])
                if similarity > threshold:
                    similar_count += 1
        
        return similar_count

    async def _normalize_platform_score(
        self, metrics: Dict[str, Any], source_id: str
    ) -> float:
        """æ ‡å‡†åŒ–ä¸åŒå¹³å°çš„çƒ­åº¦æŒ‡æ ‡"""
        # ä¸åŒå¹³å°çš„åŸºå‡†å€¼
        platform_baselines = {
            "weibo": 10000,  # å¾®åšçƒ­æœåŸºå‡†å€¼
            "zhihu": 5000,   # çŸ¥ä¹çƒ­æ¦œåŸºå‡†å€¼
            "toutiao": 8000, # å¤´æ¡çƒ­æ¦œåŸºå‡†å€¼
            "default": 1000  # é»˜è®¤åŸºå‡†å€¼
        }
        
        # è·å–åŸå§‹çƒ­åº¦æŒ‡æ ‡ï¼ˆä¸åŒå¹³å°å¯èƒ½æœ‰ä¸åŒå­—æ®µï¼‰
        original_score = 0
        if "view_count" in metrics:
            original_score = metrics["view_count"]
        elif "like_count" in metrics:
            original_score = metrics["like_count"]
        elif "comment_count" in metrics:
            original_score = metrics["comment_count"]
        elif "heat" in metrics:
            original_score = metrics["heat"]
        
        # è·å–è¯¥å¹³å°çš„åŸºå‡†å€¼
        baseline = platform_baselines.get(source_id, platform_baselines["default"])
        
        # è®¡ç®—æ ‡å‡†åŒ–å¾—åˆ†ï¼ˆ0-100èŒƒå›´ï¼‰
        normalized_score = min(original_score / baseline * 100, 100)
        
        return normalized_score

    async def _calculate_cross_source_score(
        self, title: str, all_news_items: List[Dict]
    ) -> float:
        """è®¡ç®—è·¨æºé¢‘ç‡å¾—åˆ†"""
        # è·å–åŒ…å«è¯¥æ–°é—»çš„ä¸åŒæºæ•°é‡
        sources = set()
        for item in all_news_items:
            similarity = self._calculate_title_similarity(title, item["title"])
            if similarity > 0.6:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                sources.add(item.get("source_id", ""))
        
        # è®¡ç®—å¾—åˆ†ï¼ˆå‡è®¾æœ€å¤šå‡ºç°åœ¨10ä¸ªæºä¸­ä¸ºæ»¡åˆ†ï¼‰
        score = min(len(sources) / 10 * 100, 100)
        
        return score

    async def _get_source_weight(self, source_id: str, session: AsyncSession) -> float:
        """è·å–æ¥æºæƒé‡"""
        # å¯ä»¥ä»æ•°æ®åº“æˆ–é…ç½®è·å–æ¥æºæƒé‡
        # è¿™é‡Œç®€åŒ–ä¸ºä¸€ä¸ªå›ºå®šçš„æƒé‡æ˜ å°„
        source_weights = {
            "weibo": 90,
            "zhihu": 85,
            "toutiao": 80,
            "sina": 75,
            "163": 70,
            "qq": 70,
            "sohu": 65,
            "ifeng": 65,
            "baidu": 90,
            "default": 50,
        }
        
        return source_weights.get(source_id, source_weights["default"])

    async def calculate_heat_score(
        self, 
        news_item: Dict[str, Any], 
        all_news_items: List[Dict[str, Any]],
        session: AsyncSession
    ) -> NewsHeatScore:
        """ä¸ºå•ä¸ªæ–°é—»é¡¹è®¡ç®—çƒ­åº¦åˆ†æ•°"""
        # æå–å…³é”®è¯
        keywords = await self._extract_keywords(
            news_item["title"], news_item.get("content", "")
        )
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦å¾—åˆ†ï¼ˆé€šè¿‡æœç´¢ç›¸ä¼¼æ–°é—»ï¼‰
        similar_count = 0
        for keyword in keywords[:3]:  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
            try:
                search_response = await self.heatlink_client.get(
                    "news", params={"search": keyword["word"]}
                )
                if search_response and "items" in search_response:
                    similar_count += len(search_response["items"])
            except Exception as e:
                logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
        
        # å½’ä¸€åŒ–ç›¸ä¼¼æ–°é—»æ•°é‡ä¸º0-100åˆ†
        keyword_score = min(similar_count / BASELINE_FACTOR * 100, 100)
        
        # è®¡ç®—æ—¶æ•ˆæ€§å¾—åˆ†
        # æ ‡å‡†åŒ–æ—¶é—´æ ¼å¼å¤„ç†ï¼Œç¡®ä¿æ—¶åŒºä¿¡æ¯æ­£ç¡®
        try:
            published_str = news_item["published_at"]
            # å¤„ç†ä¸åŒæ ¼å¼çš„ISOæ—¥æœŸå­—ç¬¦ä¸²
            if 'Z' in published_str:
                # æ›¿æ¢Zä¸º+00:00æ ‡å‡†æ ¼å¼
                published_str = published_str.replace('Z', '+00:00')
            elif '+' not in published_str and '-' not in published_str[10:]:
                # å¦‚æœå­—ç¬¦ä¸²ä¸­æ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œæ·»åŠ UTCæ—¶åŒº
                published_str = published_str + '+00:00'
            
            # è§£ææ—¥æœŸå¹¶ç¡®ä¿æ˜¯UTCæ—¶åŒº
            published_time = datetime.fromisoformat(published_str)
            # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œæ·»åŠ UTCæ—¶åŒº
            if published_time.tzinfo is None:
                published_time = published_time.replace(tzinfo=timezone.utc)
        except Exception as e:
            logger.error(f"è§£æå‘å¸ƒæ—¶é—´å¤±è´¥: {e}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå‘å¸ƒæ—¶é—´")
            published_time = datetime.now(timezone.utc)
        
        # ä½¿ç”¨å¸¦æ—¶åŒºçš„å½“å‰æ—¶é—´æ¥è®¡ç®—æ—¶é—´å·®
        hours_passed = (datetime.now(timezone.utc) - published_time).total_seconds() / 3600
        recency_score = 100 * math.exp(-hours_passed / DECAY_FACTOR)
        
        # è®¡ç®—åŸå¹³å°çƒ­åº¦å¾—åˆ†
        platform_score = 0
        if "metrics" in news_item:
            platform_score = await self._normalize_platform_score(
                news_item["metrics"], news_item["source_id"]
            )
        
        # è®¡ç®—è·¨æºé¢‘ç‡å¾—åˆ†
        cross_source_score = await self._calculate_cross_source_score(
            news_item["title"], all_news_items
        )
        
        # è·å–æ¥æºæƒé‡
        source_weight = await self._get_source_weight(news_item["source_id"], session)
        
        # ç»¼åˆè®¡ç®—æœ€ç»ˆçƒ­åº¦
        final_score = (
            (W_KEYWORD * keyword_score) +
            (W_RECENCY * recency_score) +
            (W_PLATFORM * platform_score) +
            (W_CROSS_SOURCE * cross_source_score) +
            (W_SOURCE * source_weight)
        )
        
        # å½’ä¸€åŒ–åˆ°0-100
        final_score = min(max(final_score, 0), 100)
        
        # åˆ›å»ºçƒ­åº¦è¯„åˆ†å¯¹è±¡
        heat_score = HeatScoreCreate(
            news_id=news_item["id"],
            source_id=news_item["source_id"],
            title=news_item["title"],
            url=news_item["url"],
            heat_score=final_score,
            relevance_score=keyword_score,
            recency_score=recency_score,
            popularity_score=platform_score,
            meta_data={
                "cross_source_score": cross_source_score,
                "source_weight": source_weight
            },
            keywords=keywords,
            published_at=published_time,
        )
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        db_obj = await news_heat_score.create(session, heat_score)
        
        return db_obj

    async def calculate_batch_heat_scores(
        self, news_items: List[Dict[str, Any]], session: AsyncSession
    ) -> Dict[str, NewsHeatScore]:
        """æ‰¹é‡è®¡ç®—çƒ­åº¦åˆ†æ•°"""
        logger.info(f"å¼€å§‹è®¡ç®—{len(news_items)}æ¡æ–°é—»çš„çƒ­åº¦åˆ†æ•°")
        
        results = {}
        for news_item in news_items:
            try:
                score_obj = await self.calculate_heat_score(
                    news_item, news_items, session
                )
                results[news_item["id"]] = score_obj
                logger.debug(f"æ–°é—»[{news_item['id']}]çƒ­åº¦è®¡ç®—å®Œæˆ: {score_obj.heat_score}")
            except Exception as e:
                logger.error(f"æ–°é—»[{news_item['id']}]çƒ­åº¦è®¡ç®—å¤±è´¥: {e}")
        
        logger.info(f"æ‰¹é‡çƒ­åº¦è®¡ç®—å®Œæˆï¼ŒæˆåŠŸ: {len(results)}, æ€»æ•°: {len(news_items)}")
        return results

    async def get_heat_scores(
        self, news_ids: List[str], session: AsyncSession
    ) -> Dict[str, float]:
        """è·å–å¤šä¸ªæ–°é—»çš„çƒ­åº¦åˆ†æ•°"""
        # å°è¯•ä»ç¼“å­˜è·å–
        # ä¸å†ä½¿ç”¨æ•´ä¸ªIDåˆ—è¡¨æ„å»ºç¼“å­˜é”®ï¼Œè€Œæ˜¯ä½¿ç”¨é•¿åº¦
        ids_count = len(news_ids)
        cache_key = f"{CACHE_PREFIX}:bulk:{uuid.uuid4().hex[:8]}:{ids_count}"
        
        cached_data = await redis_manager.get(cache_key)
        if cached_data:
            logger.debug(f"ä»ç¼“å­˜è·å–çƒ­åº¦åˆ†æ•°: {ids_count} æ¡")
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        logger.debug(f"ä»æ•°æ®åº“è·å–çƒ­åº¦åˆ†æ•°ï¼Œè¯·æ±‚ {ids_count} æ¡è®°å½•")
        scores_map = await news_heat_score.get_multi_by_news_ids(session, news_ids)
        
        # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
        result = {}
        for news_id in news_ids:
            if news_id in scores_map:
                result[news_id] = scores_map[news_id].heat_score
            else:
                result[news_id] = 0  # é»˜è®¤åˆ†æ•°
        
        # ç¼“å­˜ç»“æœ
        await redis_manager.set(cache_key, result, expire=CACHE_TTL)
        logger.debug(f"å·²å®Œæˆçƒ­åº¦åˆ†æ•°æŸ¥è¯¢ï¼Œè¿”å› {len(result)} æ¡ç»“æœ")
        
        return result

    async def get_detailed_heat_scores(
        self, news_ids: List[str], session: AsyncSession
    ) -> Dict[str, Any]:
        """è·å–å¤šä¸ªæ–°é—»çš„è¯¦ç»†çƒ­åº¦æ•°æ®"""
        # å°è¯•ä»ç¼“å­˜è·å–
        ids_count = len(news_ids)
        cache_key = f"{CACHE_PREFIX}:detailed:{uuid.uuid4().hex[:8]}:{ids_count}"
        
        cached_data = await redis_manager.get(cache_key)
        if cached_data:
            logger.debug(f"ä»ç¼“å­˜è·å–è¯¦ç»†çƒ­åº¦æ•°æ®: {ids_count} æ¡")
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        logger.debug(f"ä»æ•°æ®åº“è·å–è¯¦ç»†çƒ­åº¦æ•°æ®ï¼Œè¯·æ±‚ {ids_count} æ¡è®°å½•")
        scores_map = await news_heat_score.get_multi_by_news_ids(session, news_ids)
        
        # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
        result = {}
        for news_id in news_ids:
            if news_id in scores_map:
                result[news_id] = scores_map[news_id].to_dict()
        
        # ç¼“å­˜ç»“æœ
        await redis_manager.set(cache_key, result, expire=CACHE_TTL)
        logger.debug(f"å·²å®Œæˆè¯¦ç»†çƒ­åº¦æ•°æ®æŸ¥è¯¢ï¼Œè¿”å› {len(result)} æ¡ç»“æœ")
        
        return result

    async def get_top_news(
        self, 
        limit: int = 50, 
        skip: int = 0, 
        min_score: Optional[float] = 50.0,
        max_age_hours: Optional[int] = 72,
        session: AsyncSession = None,
    ) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨æ–°é—»åˆ—è¡¨"""
        try:
            logger.info(f"è·å–çƒ­é—¨æ–°é—»åˆ—è¡¨: limit={limit}, skip={skip}, min_score={min_score}, max_age_hours={max_age_hours}")
            
            # ä½¿ç”¨æ–°çš„å­—å…¸è¿”å›æ–¹æ³•ï¼Œé¿å… ORM æ¨¡å‹å’Œç›¸å…³å¼‚æ­¥é—®é¢˜
            news_list = await news_heat_score.get_top_news_as_dict(
                session, limit, skip, min_score, max_age_hours
            )
            
            logger.info(f"æˆåŠŸè·å–çƒ­é—¨æ–°é—»åˆ—è¡¨ï¼Œå…± {len(news_list)} æ¡è®°å½•")
            return news_list
        except Exception as e:
            logger.error(f"è·å–çƒ­é—¨æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}")
            # è®°å½•æ›´å¤šçš„é”™è¯¯ä¿¡æ¯ï¼Œæœ‰åŠ©äºè°ƒè¯•
            import traceback
            logger.error(traceback.format_exc())
            raise

    async def fetch_all_news_from_sources(self, sources: List[Dict]) -> List[Dict]:
        """ä»æ‰€æœ‰æºè·å–æ–°é—»"""
        logger.info(f"å¼€å§‹ä»{len(sources)}ä¸ªæºè·å–æ–°é—»")
        
        # é™åˆ¶å¹¶å‘è¯·æ±‚æ•°é‡
        max_concurrent = 5
        all_news_items = []
        
        # åˆ†æ‰¹å¤„ç†æºï¼Œé¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
        for i in range(0, len(sources), max_concurrent):
            batch_sources = sources[i:i+max_concurrent]
            tasks = []
            
            for source in batch_sources:
                # é€‚é…ä¸åŒçš„APIè¿”å›æ ¼å¼ï¼Œå°è¯•å¤šç§å¯èƒ½çš„IDå­—æ®µå
                source_id = None
                for id_field in ["source_id", "id", "key", "name"]:
                    if id_field in source:
                        source_id = source[id_field]
                        break
                
                if not source_id:
                    logger.warning(f"è·³è¿‡æ²¡æœ‰IDçš„æº: {source}")
                    continue
                
                task = asyncio.create_task(
                    self.heatlink_client.get(f"external/source/{source_id}")
                )
                tasks.append((source_id, task))
            
            # ç­‰å¾…è¿™ä¸€æ‰¹ä»»åŠ¡å®Œæˆ
            for source_id, task in tasks:
                try:
                    source_data = await task
                    news_items = []
                    
                    # å°è¯•ä»ä¸åŒçš„é”®è·å–æ–°é—»é¡¹
                    if source_data:
                        # 1. é¦–å…ˆå°è¯•ä»'news'é”®è·å–ï¼ˆå½“å‰APIæ ¼å¼ï¼‰
                        if isinstance(source_data, dict) and "news" in source_data:
                            news_items = source_data["news"]
                            logger.debug(f"ä»æº[{source_id}]çš„'news'é”®è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                        
                        # 2. å¦‚æœæ²¡æœ‰newsé”®ï¼Œå°è¯•ä»'items'é”®è·å–ï¼ˆæ—§æ ¼å¼ï¼‰
                        elif isinstance(source_data, dict) and "items" in source_data:
                            news_items = source_data["items"]
                            logger.debug(f"ä»æº[{source_id}]çš„'items'é”®è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                        
                        # 3. å¦‚æœAPIç›´æ¥è¿”å›äº†åˆ—è¡¨
                        elif isinstance(source_data, list):
                            news_items = source_data
                            logger.debug(f"æº[{source_id}]ç›´æ¥è¿”å›åˆ—è¡¨ï¼ŒåŒ…å« {len(news_items)} æ¡æ–°é—»")
                        
                        # ä¸ºæ¯ä¸ªæ–°é—»é¡¹æ·»åŠ source_id
                        for item in news_items:
                            item["source_id"] = source_id
                        
                        # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                        all_news_items.extend(news_items)
                    else:
                        logger.warning(f"ä»æº[{source_id}]è·å–åˆ°ç©ºæ•°æ®")
                except Exception as e:
                    logger.error(f"ä»æº[{source_id}]è·å–æ–°é—»å¤±è´¥: {e}")
        
        logger.info(f"å…±è·å–åˆ° {len(all_news_items)} æ¡æ–°é—»")
        return all_news_items

    async def update_all_heat_scores(self, session: AsyncSession):
        """æ›´æ–°æ‰€æœ‰æ–°é—»çƒ­åº¦åˆ†æ•°"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰æ–°é—»çƒ­åº¦åˆ†æ•°")
        
        try:
            # 1. è·å–æ‰€æœ‰æ–°é—»æº
            sources_data = await self.heatlink_client.get_sources(force_update=True)
            
            # å¤„ç†APIè¿”å›å€¼å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸çš„æƒ…å†µ
            if isinstance(sources_data, dict):
                sources = sources_data.get("sources", [])
            else:
                # å¦‚æœAPIç›´æ¥è¿”å›åˆ—è¡¨ï¼Œå°±ç›´æ¥ä½¿ç”¨
                sources = sources_data
            
            # 2. è·å–æ‰€æœ‰æ–°é—»
            all_news_items = await self.fetch_all_news_from_sources(sources)
            
            # 3. è®¡ç®—çƒ­åº¦è¯„åˆ†
            heat_scores = await self.calculate_batch_heat_scores(all_news_items, session)
            
            logger.info(f"âœ¨ çƒ­åº¦åˆ†æ•°æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(heat_scores)} æ¡æ–°é—»")
            return heat_scores
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°çƒ­åº¦åˆ†æ•°å¤±è´¥: {e}")
            raise

    async def update_keyword_heat(self, session: AsyncSession):
        """æ›´æ–°å…³é”®è¯çƒ­åº¦"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°å…³é”®è¯çƒ­åº¦")
        
        try:
            # è·å–æœ€è¿‘ä¸€æ®µæ—¶é—´å†…çš„çƒ­é—¨æ–°é—»
            heat_scores = await news_heat_score.get_top_heat_scores(
                session, 
                limit=100, 
                skip=0,
                min_score=30,
                max_age_hours=24 * 7  # æœ€è¿‘ä¸€å‘¨çš„æ•°æ®
            )
            
            # æå–æ‰€æœ‰å…³é”®è¯å¹¶è®¡ç®—é¢‘ç‡
            keyword_counts = {}
            for score in heat_scores:
                for keyword_item in score.keywords:
                    word = keyword_item.get("word")
                    weight = keyword_item.get("weight", 0.5)
                    
                    if word:
                        if word not in keyword_counts:
                            keyword_counts[word] = {
                                "count": 0,
                                "total_weight": 0,
                                "total_heat": 0,
                                "sources": set()
                            }
                        
                        keyword_counts[word]["count"] += 1
                        keyword_counts[word]["total_weight"] += weight
                        keyword_counts[word]["total_heat"] += score.heat_score
                        keyword_counts[word]["sources"].add(score.source_id)
            
            # è®¡ç®—å¹¶å­˜å‚¨å…³é”®è¯çƒ­åº¦
            keyword_heat = []
            for word, data in keyword_counts.items():
                # åªå…³æ³¨å‡ºç°åœ¨å¤šä¸ªæ¥æºä¸­çš„å…³é”®è¯
                if len(data["sources"]) >= 2 and data["count"] >= 3:
                    # çƒ­åº¦è®¡ç®—å…¬å¼ = å‡ºç°æ¬¡æ•° * å¹³å‡æƒé‡ * å¹³å‡çƒ­åº¦ * æ¥æºæ•°é‡
                    heat = (
                        data["count"] * 
                        (data["total_weight"] / data["count"]) * 
                        (data["total_heat"] / data["count"]) * 
                        len(data["sources"])
                    ) / 1000  # å½’ä¸€åŒ–
                    
                    keyword_heat.append({
                        "keyword": word,
                        "heat": min(heat, 100),  # ä¸Šé™100
                        "count": data["count"],
                        "sources": list(data["sources"]),
                        "updated_at": datetime.now(timezone.utc).isoformat()
                    })
            
            # æ›´æ–°åˆ°Redisç¼“å­˜
            if keyword_heat:
                # æŒ‰çƒ­åº¦æ’åº
                keyword_heat.sort(key=lambda x: x["heat"], reverse=True)
                # åªä¿ç•™å‰100ä¸ª
                top_keywords = keyword_heat[:100]
                
                # å­˜å‚¨åˆ°Redis
                cache_key = f"{CACHE_PREFIX}:keywords"
                await redis_manager.set(cache_key, top_keywords, expire=CACHE_TTL * 2)
                
                logger.info(f"âœ¨ å…³é”®è¯çƒ­åº¦æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(top_keywords)} ä¸ªå…³é”®è¯")
                return top_keywords
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°è¶³å¤Ÿçš„å…³é”®è¯æ•°æ®")
                return []
        
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å…³é”®è¯çƒ­åº¦å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    async def update_source_weights(self, session: AsyncSession):
        """æ›´æ–°æ¥æºæƒé‡"""
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°æ¥æºæƒé‡")
        
        try:
            # è·å–æ‰€æœ‰æ–°é—»æº
            sources_data = await self.heatlink_client.get_sources(force_update=True)
            
            # å¤„ç†APIè¿”å›å€¼å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸çš„æƒ…å†µ
            if isinstance(sources_data, dict):
                sources = sources_data.get("sources", [])
            else:
                # å¦‚æœAPIç›´æ¥è¿”å›åˆ—è¡¨ï¼Œå°±ç›´æ¥ä½¿ç”¨
                sources = sources_data
            
            logger.info(f"ğŸ“Š æˆåŠŸè·å–åˆ° {len(sources)} ä¸ªæ–°é—»æº")
            
            # åˆå§‹åŒ–æ¥æºæƒé‡æ•°æ®
            source_stats = {}
            
            # ä¸ºæ¯ä¸ªæ¥æºè·å–æœ€è¿‘çš„æ–°é—»
            for source in sources:
                source_id = source.get("source_id") or source.get("id")
                if not source_id:
                    logger.warning("âš ï¸ è·³è¿‡æ²¡æœ‰æœ‰æ•ˆIDçš„æº")
                    continue
                    
                try:
                    # è·å–è¯¥æ¥æºçš„æœ€æ–°æ–°é—»
                    source_news = await self.heatlink_client.get_source(source_id, force_update=True)
                    
                    # å°è¯•ä»ä¸åŒçš„é”®è·å–æ–°é—»é¡¹
                    news_items = []
                    
                    # å…ˆå°è¯•ä»'news'é”®è·å–ï¼Œè¿™æ˜¯APIå½“å‰çš„æ ¼å¼
                    if isinstance(source_news, dict) and "news" in source_news:
                        news_items = source_news.get("news", [])
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå†å°è¯•ä»'items'é”®è·å–ï¼ˆæ—§æ ¼å¼çš„å…¼å®¹ï¼‰
                    elif isinstance(source_news, dict) and "items" in source_news:
                        news_items = source_news.get("items", [])
                    
                    # æˆ–è€…APIç›´æ¥è¿”å›äº†åˆ—è¡¨
                    elif isinstance(source_news, list):
                        news_items = source_news
                    
                    # ç»Ÿè®¡è¯¥æ¥æºçš„æ•°æ®
                    if news_items:
                        avg_engagement = 0
                        total_items = len(news_items)
                        
                        # åˆ†ææ–°é—»é¡¹çš„äº’åŠ¨æ•°æ®
                        for item in news_items:
                            metrics = item.get("metrics", {})
                            engagement = (
                                metrics.get("view_count", 0) + 
                                metrics.get("like_count", 0) * 3 + 
                                metrics.get("comment_count", 0) * 5 + 
                                metrics.get("share_count", 0) * 10
                            )
                            avg_engagement += engagement
                        
                        if total_items > 0:
                            avg_engagement /= total_items
                        
                        # è®¡ç®—æ›´æ–°é¢‘ç‡åˆ†æ•°ï¼ˆåŸºäºæœ€æ–°æ–‡ç« çš„æ—¶é—´é—´éš”ï¼‰
                        update_frequency = 50  # é»˜è®¤å€¼
                        if total_items >= 2:
                            try:
                                # è·å–æœ€æ–°ä¸¤ç¯‡æ–‡ç« çš„æ—¶é—´å·®
                                latest_str = news_items[0]["published_at"]
                                second_latest_str = news_items[1]["published_at"]
                                
                                # å¤„ç†ä¸åŒæ ¼å¼çš„ISOæ—¥æœŸå­—ç¬¦ä¸² - æœ€æ–°æ–‡ç« 
                                if 'Z' in latest_str:
                                    latest_str = latest_str.replace('Z', '+00:00')
                                elif '+' not in latest_str and '-' not in latest_str[10:]:
                                    latest_str = latest_str + '+00:00'
                                
                                # å¤„ç†ä¸åŒæ ¼å¼çš„ISOæ—¥æœŸå­—ç¬¦ä¸² - ç¬¬äºŒæ–°æ–‡ç« 
                                if 'Z' in second_latest_str:
                                    second_latest_str = second_latest_str.replace('Z', '+00:00')
                                elif '+' not in second_latest_str and '-' not in second_latest_str[10:]:
                                    second_latest_str = second_latest_str + '+00:00'
                                    
                                # è§£ææ—¥æœŸå¹¶ç¡®ä¿æ˜¯UTCæ—¶åŒº
                                latest_time = datetime.fromisoformat(latest_str)
                                second_latest_time = datetime.fromisoformat(second_latest_str)
                                
                                # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œæ·»åŠ UTCæ—¶åŒº
                                if latest_time.tzinfo is None:
                                    latest_time = latest_time.replace(tzinfo=timezone.utc)
                                if second_latest_time.tzinfo is None:
                                    second_latest_time = second_latest_time.replace(tzinfo=timezone.utc)
                                
                                # è®¡ç®—å°æ—¶å·®
                                hours_diff = (latest_time - second_latest_time).total_seconds() / 3600
                                
                                # æ›´æ–°è¶Šé¢‘ç¹ï¼Œåˆ†æ•°è¶Šé«˜ï¼ˆæœ€é«˜100ï¼‰
                                if hours_diff <= 1:  # æ¯å°æ—¶æ›´æ–°
                                    update_frequency = 100
                                elif hours_diff <= 3:  # æ¯3å°æ—¶æ›´æ–°
                                    update_frequency = 90
                                elif hours_diff <= 6:  # æ¯6å°æ—¶æ›´æ–°
                                    update_frequency = 80
                                elif hours_diff <= 12:  # æ¯12å°æ—¶æ›´æ–°
                                    update_frequency = 70
                                elif hours_diff <= 24:  # æ¯å¤©æ›´æ–°
                                    update_frequency = 60
                                else:
                                    update_frequency = 50
                            except Exception as e:
                                logger.warning(f"âš ï¸ è®¡ç®—æ›´æ–°é¢‘ç‡å¤±è´¥: {e}")
                                import traceback
                                logger.debug(traceback.format_exc())
                        
                        # ç»¼åˆè®¡ç®—æ¥æºæƒé‡
                        source_weight = min(
                            (avg_engagement / 1000) * 0.7 + update_frequency * 0.3, 
                            100
                        )
                        
                        # å­˜å‚¨æ¥æºç»Ÿè®¡æ•°æ®
                        source_stats[source_id] = {
                            "weight": source_weight,
                            "avg_engagement": avg_engagement,
                            "update_frequency": update_frequency,
                            "item_count": total_items,
                            "updated_at": datetime.now(timezone.utc).isoformat()
                        }
                        logger.debug(f"âœ… æˆåŠŸå¤„ç†æº '{source_id}', æƒé‡={source_weight:.2f}")
                    else:
                        logger.warning(f"âš ï¸ æº '{source_id}' æ²¡æœ‰è¿”å›æ–°é—»æ•°æ®")
                
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ¥æº {source_id} å¤±è´¥: {e}")
            
            # å­˜å‚¨æ‰€æœ‰æ¥æºæƒé‡åˆ°Redis
            if source_stats:
                cache_key = f"{CACHE_PREFIX}:source_weights"
                await redis_manager.set(cache_key, source_stats, expire=CACHE_TTL * 24)  # ç¼“å­˜24å°æ—¶
                
                logger.info(f"âœ¨ æ¥æºæƒé‡æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {len(source_stats)} ä¸ªæ¥æº")
                return source_stats
            else:
                logger.warning("âš ï¸ æœªè·å–åˆ°æœ‰æ•ˆçš„æ¥æºæ•°æ®")
                return {}
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¥æºæƒé‡å¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            raise


# åˆ›å»ºæœåŠ¡å®ä¾‹
heat_score_service = NewsHeatScoreService() 